{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ratapakorn/chatbot-profile/blob/main/nb/Gemma3_(1B)-GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oJEZ943DOgd"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3v4aVl6DOgj"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGeD9LTdDOgk"
      },
      "source": [
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSgOIHGODOgk"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hWlSCEDrDOgl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install or uv pip install\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vwRpROuxDOgn"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd2cpn1kaXRA"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rezoBIE1aXRA"
      },
      "source": [
        "Load up `Gemma 3 1B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "8965316c-95b1-49f1-9c7c-4292f4bbc3b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth cannot find any torch accelerator? You need a GPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20699311.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m fourbit_models = [\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m#         except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0munsloth_zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo` then retry!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# Get device types and other variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m from .device_type import (\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mis_hip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mget_device_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/device_type.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA, AMD and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m# HIP fails for autocast and other torch functions. Use CUDA instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mDEVICE_TYPE_TORCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/device_type.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accelerator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth cannot find any torch accelerator? You need a GPU.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0maccelerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccelerator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth cannot find any torch accelerator? You need a GPU."
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "max_seq_length = 1024\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-1b-it\",\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nHQVsQJ2roh"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNuwc5sJ2pYK"
      },
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 8,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We're using OpenAI's famous GSM8K dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEibULDtlOMU"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"openai/gsm8k\", \"main\", split = \"train\")\n",
        "# dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"labels.jsonl\",  # one JSON object per line\n",
        "    split=\"train\"\n",
        ")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRfa3z_atGgT"
      },
      "source": [
        "Let's look at the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIRsNJ_YitXl"
      },
      "outputs": [],
      "source": [
        "str(dataset[0][\"user\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3yJTiFgtKYq"
      },
      "outputs": [],
      "source": [
        "dataset[0][\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqq9kGuVtIYn"
      },
      "source": [
        "We notice all answers like about have a ####, so we extract it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5I3BCkViwuC"
      },
      "outputs": [],
      "source": [
        "def extract_hash_answer(text):\n",
        "    if \"####\" not in text: return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "extract_hash_answer(dataset[0][\"response\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiFNBLyytPCD"
      },
      "source": [
        "We now create a system prompt which can be customized. We add 4 extra symbols for working out or thinking / reasoning sections and a final answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHjiV3kGi8Y9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"ill give you a log like this and you pick which ai model to use\n",
        "\n",
        "cnn, random forest, isolation forest, naivebayes, bert, autoencoder, dqn, other\n",
        "\n",
        "(PICK ONLY ONE) and a short one sentence reasoning\n",
        "\n",
        "The format is <AI Model> - <Reasoning>\n",
        "\"\"\"\n",
        "system_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFRYlk9ntYTm"
      },
      "source": [
        "Let's map the dataset! and see the first row:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = str(system_prompt)\n",
        "\n",
        "system_prompt"
      ],
      "metadata": {
        "id": "NVg9GLZoGRaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import Features, Sequence, Value\n",
        "\n",
        "# features = Features({\n",
        "#     \"prompt\": Sequence({\"role\": Value(\"string\"), \"content\": Value(\"string\")}),\n",
        "#     \"answer\": Value(\"string\"),\n",
        "# })\n",
        "\n",
        "# dataset = dataset.map(\n",
        "#     lambda x: {\n",
        "#         \"prompt\": [\n",
        "#             {\"role\": \"system\", \"content\": (system_prompt)},\n",
        "#             {\"role\": \"user\",   \"content\": '\"' + str(x.get(\"user\", \"\")).replace('\"', '\\\\\"') + '\"'},\n",
        "#         ],\n",
        "#         \"answer\": (x.get(\"response\", \"\")),\n",
        "#     },\n",
        "#     features=features,\n",
        "#     remove_columns=dataset.column_names,\n",
        "# )\n",
        "\n",
        "# dataset[0]\n"
      ],
      "metadata": {
        "id": "Fxnpdth6F9lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tkTF5Hmlhl-"
      },
      "outputs": [],
      "source": [
        "# dataset = dataset.map(lambda x: {\n",
        "#     \"prompt\" : [\n",
        "#         {\"role\": \"system\", \"content\": system_prompt},\n",
        "#         {\"role\": \"user\",   \"content\": x[\"user\"]},\n",
        "#     ],\n",
        "#     \"answer\": x[\"response\"],\n",
        "# })\n",
        "# str(dataset[0])\n",
        "\n",
        "#original\n",
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": str(x[\"user\"])},\n",
        "    ],\n",
        "    \"answer\": (x[\"response\"]),\n",
        "})\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6MsfbGUtja0"
      },
      "source": [
        "We create a regex format to match the reasoning sections and answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5X6oDNDn6Zj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\\\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME3-UVc6tnYP"
      },
      "source": [
        "We verify it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVvrKUBEtoQD"
      },
      "outputs": [],
      "source": [
        "match_format.search(\n",
        "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
        "    \"<SOLUTION>2</SOLUTION>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qglh2OxpuQzK"
      },
      "source": [
        "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8MPYPvvo1ri"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqnEZ4msuZyZ"
      },
      "source": [
        "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LlYVZjdpij9"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
        "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
        "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Helper: extract whatever the model put as its final answer (text)\n",
        "# 1) Prefer content between your tags:  <solution_start> ... <solution_end>\n",
        "# 2) Otherwise fall back to a line like: \"#### <answer>\"\n",
        "FINAL_LINE_RE = re.compile(r\"(?m)^\\s*####\\s*(?P<ans>[^\\n]+?)\\s*$\")\n",
        "ONLY_NUMBER_RE = re.compile(r\"^\\s*[-+]?\\d+(?:\\.\\d+)?\\s*$\")\n",
        "\n",
        "def extract_solution_text(response: str) -> str:\n",
        "    if solution_start in response and solution_end in response:\n",
        "        try:\n",
        "            return response.split(solution_start, 1)[1].split(solution_end, 1)[0].strip()\n",
        "        except Exception:\n",
        "            pass\n",
        "    m = FINAL_LINE_RE.search(response)\n",
        "    return m.group(\"ans\").strip() if m else \"\"\n",
        "\n",
        "def is_text_answer(ans: str) -> bool:\n",
        "    # Accept if it has at least one letter and is not just a number\n",
        "    return bool(ans) and re.search(r\"[A-Za-z]\", ans) and not ONLY_NUMBER_RE.match(ans)\n",
        "\n",
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        response = completion[0][\"content\"]\n",
        "        ans = extract_solution_text(response)\n",
        "        score = 3.0 if is_text_answer(ans) else 0.0\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        response = completion[0][\"content\"]\n",
        "        score = 0.0\n",
        "\n",
        "        # keep your tag-count heuristics\n",
        "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
        "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
        "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
        "\n",
        "        # new: text-specific checks for the extracted final answer\n",
        "        ans = extract_solution_text(response)\n",
        "        if is_text_answer(ans):\n",
        "            score += 0.5  # good: looks like text, not just a number\n",
        "            n_words = len(ans.split())\n",
        "            if 1 <= n_words <= 40:\n",
        "                score += 0.5   # concise final text\n",
        "            elif n_words > 120:\n",
        "                score -= 0.5   # overly long for a \"final answer\"\n",
        "        else:\n",
        "            score -= 0.5       # missing/empty or purely numeric\n",
        "\n",
        "        scores.append(score)\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "d4Eatz-SJpja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBwDVDxtuhWm"
      },
      "source": [
        "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnKYp_IYqFr2"
      },
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Correct answer gets 3 points!\n",
        "        if guess == true_answer:\n",
        "            score += 3.0\n",
        "        # Match if spaces are seen\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 1.5\n",
        "        else:\n",
        "            # We also reward it if the answer is close via ratios!\n",
        "            # Ie if the answer is within some range, reward it!\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
        "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
        "                else: score -= 1.0 # Penalize wrong answers\n",
        "            except:\n",
        "                score -= 0.5 # Penalize\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nrQa4msuJYsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvOYCf1Ly83w"
      },
      "source": [
        "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtFAX3_xy77b"
      },
      "outputs": [],
      "source": [
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqWply0z0DrP"
      },
      "outputs": [],
      "source": [
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            guess       = float(guess.strip())\n",
        "            scores.append(1.5 if guess == true_answer else 0.0)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, difflib\n",
        "from fractions import Fraction\n",
        "\n",
        "# --- helpers ---\n",
        "_OnlyNum = re.compile(r'^\\s*[-+]?(\\d+(?:[,_]\\d{3})*|\\d+)(?:\\.\\d+)?(?:[eE][-+]?\\d+)?\\s*%?\\s*$')\n",
        "\n",
        "def _normalize_text(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    # collapse whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    # drop most punctuation (keep % for possible semantics)\n",
        "    s = re.sub(r'[^\\w\\s%]', '', s)  # remove punctuation except %; adjust if needed\n",
        "    return s.strip()\n",
        "\n",
        "def _text_similarity(a: str, b: str) -> float:\n",
        "    # similarity on normalized strings\n",
        "    a_n, b_n = _normalize_text(a), _normalize_text(b)\n",
        "    return difflib.SequenceMatcher(None, a_n, b_n).ratio()\n",
        "\n",
        "def _to_float(s: str):\n",
        "    if s is None:\n",
        "        return None\n",
        "    t = s.strip()\n",
        "    if not t:\n",
        "        return None\n",
        "    # percentage?\n",
        "    is_pct = t.endswith('%')\n",
        "    if is_pct:\n",
        "        t = t[:-1]\n",
        "\n",
        "    # fractions like \"1/2\"\n",
        "    if '/' in t and not re.search(r'[A-Za-z]', t):\n",
        "        try:\n",
        "            val = float(Fraction(t.replace(' ', '')))\n",
        "            return val / 100.0 if is_pct else val\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # remove thousands separators/underscores/spaces\n",
        "    t = t.replace(',', '').replace('_', '').replace(' ', '')\n",
        "\n",
        "    try:\n",
        "        val = float(t)\n",
        "        return val / 100.0 if is_pct else val\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# --- TEXT ANSWER CHECKER ---\n",
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    # Extract candidate answers using your existing regex `match_format`\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted = [\n",
        "        m.group(1) if (m := match_format.search(r)) is not None else None\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # 1) exact match (verbatim)\n",
        "        if guess == true_answer:\n",
        "            scores.append(3.0)\n",
        "            continue\n",
        "\n",
        "        # 2) whitespace-only differences\n",
        "        if guess.strip() == true_answer.strip():\n",
        "            scores.append(2.0)\n",
        "            continue\n",
        "\n",
        "        # 3) case/punct/spacing-insensitive equality\n",
        "        if _normalize_text(guess) == _normalize_text(true_answer):\n",
        "            scores.append(1.5)\n",
        "            continue\n",
        "\n",
        "        # 4) fuzzy similarity on normalized text\n",
        "        sim = _text_similarity(guess, true_answer)\n",
        "        if   sim >= 0.90: scores.append(1.0)\n",
        "        elif sim >= 0.80: scores.append(0.5)\n",
        "        else:             scores.append(-0.5)  # penalize clear mismatch\n",
        "\n",
        "    return scores\n",
        "\n",
        "# --- NUMERIC ANSWER CHECKER (more robust parsing) ---\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    # Extract numeric-looking substrings using your existing `match_numbers`\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted = [\n",
        "        m.group(1) if (m := match_numbers.search(r)) is not None else None\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        try:\n",
        "            g = _to_float(guess)\n",
        "            t = _to_float(true_answer if isinstance(true_answer, str) else str(true_answer))\n",
        "            if g is None or t is None:\n",
        "                scores.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # exact numeric equality\n",
        "            if g == t:\n",
        "                scores.append(1.5)\n",
        "                continue\n",
        "\n",
        "            # relative tolerance buckets\n",
        "            # avoid division by zero: use max(|t|, 1e-12)\n",
        "            denom = max(abs(t), 1e-12)\n",
        "            rel_err = abs(g - t) / denom\n",
        "\n",
        "            if   rel_err <= 0.01: scores.append(1.0)   # within 1%\n",
        "            elif rel_err <= 0.05: scores.append(0.75)  # within 5%\n",
        "            elif rel_err <= 0.10: scores.append(0.5)   # within 10%\n",
        "            else:                  scores.append(0.0)\n",
        "        except Exception:\n",
        "            scores.append(0.0)\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "PQdG2neTJztg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# --- helpers ---------------------------------------------------------------\n",
        "\n",
        "_PAIR_LINE = re.compile(\n",
        "    r'^\\s*[“\"]?(?P<model>[^-–—:|]+?)\\s*[-–—:|]\\s*(?P<reason>.+?)\\s*[”\"]?\\s*$'\n",
        ")\n",
        "\n",
        "def _normalize(s: str) -> str:\n",
        "    \"\"\"Lowercase, trim, collapse whitespace.\"\"\"\n",
        "    s = re.sub(r'\\s+', ' ', s or '').strip()\n",
        "    return s.casefold()\n",
        "\n",
        "def _similar(a: str, b: str) -> float:\n",
        "    \"\"\"Similarity in [0,1] using SequenceMatcher.\"\"\"\n",
        "    return SequenceMatcher(None, _normalize(a), _normalize(b)).ratio()\n",
        "\n",
        "def _extract_pair(text: str):\n",
        "    \"\"\"\n",
        "    Return (model, reason) by scanning lines and grabbing the first\n",
        "    'X - Y' (or –, —, :, |) pattern. If none, return (None, None).\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None, None\n",
        "    for line in text.splitlines():\n",
        "        m = _PAIR_LINE.match(line)\n",
        "        if m:\n",
        "            return m.group('model').strip(), m.group('reason').strip()\n",
        "    return None, None\n",
        "\n",
        "def _coerce_answer_pair(ans):\n",
        "    \"\"\"\n",
        "    Accept answers as:\n",
        "      - \"Model - Reason\" string\n",
        "      - (\"Model\", \"Reason\") tuple/list\n",
        "      - {\"model\": \"...\", \"reasoning\": \"...\"} or {\"model\": \"...\", \"reason\": \"...\"}\n",
        "    \"\"\"\n",
        "    if isinstance(ans, (list, tuple)) and len(ans) == 2:\n",
        "        return str(ans[0]), str(ans[1])\n",
        "    if isinstance(ans, dict):\n",
        "        model = ans.get('model') or ans.get('ai_model') or ans.get('name')\n",
        "        reason = ans.get('reasoning') or ans.get('reason') or ans.get('rationale')\n",
        "        return (str(model) if model is not None else None,\n",
        "                str(reason) if reason is not None else None)\n",
        "    if isinstance(ans, str):\n",
        "        return _extract_pair(ans)\n",
        "    # Fallback\n",
        "    return None, None\n",
        "\n",
        "# --- main -----------------------------------------------------------------\n",
        "\n",
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    \"\"\"\n",
        "    Scores completions against expected 'AI Model - Reasoning' pairs.\n",
        "\n",
        "    Inputs:\n",
        "      - prompts: unused here but kept for API compatibility.\n",
        "      - completions: list like [[{\"content\": \"...\"}], ...]\n",
        "      - answer: list of expected pairs; each item can be:\n",
        "          * \"Model - Reasoning\"\n",
        "          * (\"Model\", \"Reasoning\")\n",
        "          * {\"model\": \"...\", \"reasoning\": \"...\"} (keys are flexible)\n",
        "    Output:\n",
        "      - List[float] of scores, one per completion (max 3.0).\n",
        "    \"\"\"\n",
        "    # Flatten completion texts\n",
        "    texts = [c[0][\"content\"] if c and isinstance(c[0], dict) else \"\" for c in completions]\n",
        "\n",
        "    # Extract predicted pairs\n",
        "    preds = [_extract_pair(t) for t in texts]\n",
        "\n",
        "    # Coerce ground-truth pairs\n",
        "    truths = [_coerce_answer_pair(a) for a in answer]\n",
        "\n",
        "    scores = []\n",
        "    for (pred_model, pred_reason), (true_model, true_reason) in zip(preds, truths):\n",
        "        # If extraction failed or truth missing, score 0\n",
        "        if not pred_model or not pred_reason or not true_model or not true_reason:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # Similarity per field\n",
        "        sm = _similar(pred_model, true_model)\n",
        "        sr = _similar(pred_reason, true_reason)\n",
        "\n",
        "        # Convert similarities to points (max 1.5 per field)\n",
        "        def field_points(sim: float) -> float:\n",
        "            if sim >= 0.999:     # effectively exact after normalization\n",
        "                return 1.5\n",
        "            elif sim >= 0.90:\n",
        "                return 1.0\n",
        "            elif sim >= 0.80:\n",
        "                return 0.5\n",
        "            elif sim >= 0.65:\n",
        "                return 0.25\n",
        "            else:\n",
        "                return -0.25\n",
        "\n",
        "        total = field_points(sm) + field_points(sr)\n",
        "\n",
        "        # Clamp to [0, 3]\n",
        "        total = max(0.0, min(3.0, total))\n",
        "        scores.append(total)\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "qsRV6mPVoApA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "_PAIR_LINE = re.compile(r'^\\s*[“\"]?(?P<model>[^-–—:|]+?)\\s*[-–—:|]\\s*(?P<reason>.+?)\\s*[”\"]?\\s*$')\n",
        "\n",
        "def _normalize(s):\n",
        "    import re\n",
        "    return re.sub(r'\\s+', ' ', (s or '')).strip().casefold()\n",
        "\n",
        "def _sim(a, b):\n",
        "    return SequenceMatcher(None, _normalize(a), _normalize(b)).ratio()\n",
        "\n",
        "def _extract_pair(text):\n",
        "    if not text:\n",
        "        return None, None\n",
        "    for line in str(text).splitlines():\n",
        "        m = _PAIR_LINE.match(line)\n",
        "        if m:\n",
        "            return m.group('model').strip(), m.group('reason').strip()\n",
        "    return None, None\n",
        "\n",
        "def _coerce_truth(ans):\n",
        "    if isinstance(ans, (list, tuple)) and len(ans) == 2:\n",
        "        return str(ans[0]), str(ans[1])\n",
        "    if isinstance(ans, dict):\n",
        "        m = ans.get('model') or ans.get('ai_model') or ans.get('name')\n",
        "        r = ans.get('reasoning') or ans.get('reason') or ans.get('rationale')\n",
        "        return (str(m) if m is not None else None, str(r) if r is not None else None)\n",
        "    if isinstance(ans, str):\n",
        "        return _extract_pair(ans)\n",
        "    return None, None\n",
        "\n",
        "def _to_text_list(completions):\n",
        "    out = []\n",
        "    for c in completions:\n",
        "        if isinstance(c, str):\n",
        "            out.append(c)\n",
        "        elif isinstance(c, list):\n",
        "            # e.g., [[{\"content\": \"...\"}]] or a token list; join safely\n",
        "            if c and isinstance(c[0], dict) and \"content\" in c[0]:\n",
        "                out.append(c[0][\"content\"])\n",
        "            else:\n",
        "                out.append(\" \".join([ (x.get(\"content\",\"\") if isinstance(x,dict) else str(x)) for x in c ]))\n",
        "        elif isinstance(c, dict):\n",
        "            out.append(c.get(\"content\",\"\"))\n",
        "        else:\n",
        "            out.append(str(c))\n",
        "    return out\n",
        "\n",
        "def check_answer(completions=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function for TRL GRPOTrainer.\n",
        "    Expects completions (list[str]) and a dataset column 'answer' in kwargs.\n",
        "    Returns a list[float] with the same length as completions.\n",
        "    \"\"\"\n",
        "    answers = kwargs.get(\"answer\")\n",
        "    if answers is None:\n",
        "        # No ground truth provided -> reward nothing\n",
        "        return [0.0] * (len(completions) if completions else 0)\n",
        "\n",
        "    texts = _to_text_list(completions or [])\n",
        "    preds = [_extract_pair(t) for t in texts]\n",
        "\n",
        "    # TRL usually repeats each row's columns per generated completion;\n",
        "    # just trust the length to match len(texts). If it's per-prompt, repeat.\n",
        "    if len(answers) != len(texts):\n",
        "        # broadcast per-prompt answers to per-completion if needed\n",
        "        if len(answers) == 1:\n",
        "            answers = answers * len(texts)\n",
        "        else:\n",
        "            # fallback: truncate/pad\n",
        "            answers = (answers * ((len(texts) + len(answers) - 1)//len(answers)))[:len(texts)]\n",
        "\n",
        "    truths = [_coerce_truth(a) for a in answers]\n",
        "\n",
        "    rewards = []\n",
        "    for (pm, pr), (tm, tr) in zip(preds, truths):\n",
        "        if not pm or not pr or not tm or not tr:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "        sm, sr = _sim(pm, tm), _sim(pr, tr)\n",
        "        # up to 1.5 per field → max 3.0\n",
        "        def pts(s):\n",
        "            return 1.5 if s >= 0.999 else 1.0 if s >= 0.90 else 0.5 if s >= 0.80 else 0.25 if s >= 0.65 else -0.25\n",
        "        total = max(0.0, min(3.0, pts(sm) + pts(sr)))\n",
        "        rewards.append(total)\n",
        "    return rewards\n"
      ],
      "metadata": {
        "id": "JVkanrkes64H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, difflib\n",
        "from fractions import Fraction\n",
        "\n",
        "# ====== Tag defaults (override if you use different ones) ======\n",
        "reasoning_start = \"<reasoning_start>\"\n",
        "reasoning_end   = \"</reasoning_end>\"\n",
        "solution_start  = \"<solution_start>\"\n",
        "solution_end    = \"</solution_end>\"\n",
        "\n",
        "# ====== Regexes (available if other code uses them) ======\n",
        "# Numeric extractor: first numeric-looking token (supports commas, decimals, exp, %, simple fractions)\n",
        "match_numbers = re.compile(r'(?<!\\w)([-+]?(?:\\d[\\d,._]*)?(?:\\.\\d+)?(?:[eE][-+]?\\d+)?%?|[-+]?\\d+/\\d+%?)(?!\\w)')\n",
        "\n",
        "# If some legacy code still uses `match_format.search(resp).group(1)`, prefer using the helpers below.\n",
        "# (We keep this here but the functions below do not rely on it.)\n",
        "match_format = re.compile(r'(?m)^\\s*####\\s*(?P<ans>[^\\n]+?)\\s*$')\n",
        "\n",
        "\n",
        "# ====== Helpers ======\n",
        "def extract_response_text(completion):\n",
        "    \"\"\"Return the assistant's final text from various completion shapes.\"\"\"\n",
        "    # 1) Already a string\n",
        "    if isinstance(completion, str):\n",
        "        return completion\n",
        "\n",
        "    # 2) List of chat messages\n",
        "    if isinstance(completion, list) and completion:\n",
        "        # prefer the last assistant message\n",
        "        for msg in reversed(completion):\n",
        "            if isinstance(msg, dict) and msg.get(\"role\") in (\"assistant\", \"assistant_final\"):\n",
        "                c = msg.get(\"content\")\n",
        "                if isinstance(c, str):\n",
        "                    return c\n",
        "        # fallback: last content if present\n",
        "        last = completion[-1]\n",
        "        if isinstance(last, dict) and isinstance(last.get(\"content\"), str):\n",
        "            return last[\"content\"]\n",
        "\n",
        "    # 3) Dict-like generations\n",
        "    if isinstance(completion, dict):\n",
        "        for k in (\"content\", \"generated_text\", \"text\", \"response\"):\n",
        "            v = completion.get(k)\n",
        "            if isinstance(v, str):\n",
        "                return v\n",
        "\n",
        "    # 4) Last resort\n",
        "    return str(completion)\n",
        "\n",
        "\n",
        "def _between_tags(text, start_tag, end_tag):\n",
        "    if start_tag in text and end_tag in text:\n",
        "        try:\n",
        "            return text.split(start_tag, 1)[1].split(end_tag, 1)[0].strip()\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "FINAL_LINE_RE = re.compile(r\"(?m)^\\s*####\\s*(?P<ans>[^\\n]+?)\\s*$\")\n",
        "ONLY_NUMBER_RE = re.compile(r\"^\\s*[-+]?\\d+(?:\\.\\d+)?\\s*$\")\n",
        "\n",
        "def extract_solution_text(response: str) -> str:\n",
        "    \"\"\"Best-effort final answer extraction.\"\"\"\n",
        "    # 1) Preferred: tags\n",
        "    tagged = _between_tags(response, solution_start, solution_end)\n",
        "    if tagged:\n",
        "        return tagged\n",
        "\n",
        "    # 2) Markdown final line: \"#### answer\"\n",
        "    m = FINAL_LINE_RE.search(response)\n",
        "    if m:\n",
        "        return m.group(\"ans\").strip()\n",
        "\n",
        "    # 3) Fallback: whole response\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "def _normalize_text(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    s = re.sub(r'[^\\w\\s%]', '', s)  # drop punctuation except %\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "def _text_similarity(a: str, b: str) -> float:\n",
        "    return difflib.SequenceMatcher(None, _normalize_text(a), _normalize_text(b)).ratio()\n",
        "\n",
        "\n",
        "def _to_float(s: str):\n",
        "    if s is None:\n",
        "        return None\n",
        "    t = s.strip()\n",
        "    if not t:\n",
        "        return None\n",
        "    is_pct = t.endswith('%')\n",
        "    if is_pct:\n",
        "        t = t[:-1]\n",
        "\n",
        "    # fractions like \"1/2\"\n",
        "    if '/' in t and not re.search(r'[A-Za-z]', t):\n",
        "        try:\n",
        "            val = float(Fraction(t.replace(' ', '')))\n",
        "            return val / 100.0 if is_pct else val\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    t = t.replace(',', '').replace('_', '').replace(' ', '')\n",
        "    try:\n",
        "        val = float(t)\n",
        "        return val / 100.0 if is_pct else val\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _is_text_answer(ans: str) -> bool:\n",
        "    # At least one letter & not purely numeric\n",
        "    return bool(ans) and re.search(r\"[A-Za-z]\", ans) and not ONLY_NUMBER_RE.match(ans)\n",
        "\n",
        "\n",
        "# ====== Format scorers ======\n",
        "def match_format_exactly(completions, **kwargs):\n",
        "    \"\"\"Score 3.0 if the output has exactly one pair of reasoning/solution tags and a non-empty text answer.\"\"\"\n",
        "    scores = []\n",
        "    for c in completions:\n",
        "        resp = extract_response_text(c)\n",
        "        score = 0.0\n",
        "        ok = (\n",
        "            resp.count(reasoning_start) == 1 and\n",
        "            resp.count(reasoning_end)   == 1 and\n",
        "            resp.count(solution_start)  == 1 and\n",
        "            resp.count(solution_end)    == 1\n",
        "        )\n",
        "        if ok:\n",
        "            ans = extract_solution_text(resp)\n",
        "            if _is_text_answer(ans):\n",
        "                score = 3.0\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def match_format_approximately(completions, **kwargs):\n",
        "    \"\"\"Heuristic: right number of tags + plausible final text.\"\"\"\n",
        "    scores = []\n",
        "    for c in completions:\n",
        "        resp = extract_response_text(c)\n",
        "        score = 0.0\n",
        "        score += 0.5 if resp.count(reasoning_start) == 1 else -0.5\n",
        "        score += 0.5 if resp.count(reasoning_end)   == 1 else -0.5\n",
        "        score += 0.5 if resp.count(solution_start)  == 1 else -0.5\n",
        "        score += 0.5 if resp.count(solution_end)    == 1 else -0.5\n",
        "\n",
        "        ans = extract_solution_text(resp)\n",
        "        if _is_text_answer(ans):\n",
        "            score += 0.5\n",
        "            n_words = len(ans.split())\n",
        "            if 1 <= n_words <= 40:\n",
        "                score += 0.5\n",
        "            elif n_words > 120:\n",
        "                score -= 0.5\n",
        "        else:\n",
        "            score -= 0.5\n",
        "\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "# ====== Answer checkers ======\n",
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    \"\"\"\n",
        "    Text answer scorer.\n",
        "    - If `true_answer` is a dict with keys like {\"model\": \"...\", \"reason\": \"...\"},\n",
        "      we reward presence/overlap of both parts.\n",
        "    - If it's a plain string, we do normalized + fuzzy matching.\n",
        "    \"\"\"\n",
        "    responses = [extract_response_text(c) for c in completions]\n",
        "    preds = [extract_solution_text(r) or \"\" for r in responses]\n",
        "\n",
        "    scores = []\n",
        "    for pred, true_answer in zip(preds, answer):\n",
        "        score = 0.0\n",
        "\n",
        "        # Dict form: e.g. {\"model\": \"GPT-4\", \"reason\": \"...\"}\n",
        "        if isinstance(true_answer, dict):\n",
        "            gold_model  = str(true_answer.get(\"model\", \"\")).strip()\n",
        "            gold_reason = str(true_answer.get(\"reason\", \"\")).strip()\n",
        "\n",
        "            p_norm = _normalize_text(pred)\n",
        "            m_norm = _normalize_text(gold_model)\n",
        "            r_norm = _normalize_text(gold_reason)\n",
        "\n",
        "            # model presence\n",
        "            if m_norm and (m_norm in p_norm or _text_similarity(p_norm, m_norm) >= 0.90):\n",
        "                score += 1.5\n",
        "\n",
        "            # reason similarity\n",
        "            if r_norm:\n",
        "                sim = _text_similarity(pred, gold_reason)\n",
        "                if   sim >= 0.90: score += 1.5\n",
        "                elif sim >= 0.80: score += 1.0\n",
        "                elif sim >= 0.60: score += 0.5\n",
        "\n",
        "            # small penalty if answer is suspiciously short\n",
        "            if len(pred.split()) < 2:\n",
        "                score -= 0.25\n",
        "\n",
        "        # String form\n",
        "        else:\n",
        "            true_str = str(true_answer)\n",
        "\n",
        "            if pred == true_str:\n",
        "                score += 3.0\n",
        "            elif pred.strip() == true_str.strip():\n",
        "                score += 2.0\n",
        "            elif _normalize_text(pred) == _normalize_text(true_str):\n",
        "                score += 1.5\n",
        "            else:\n",
        "                sim = _text_similarity(pred, true_str)\n",
        "                if   sim >= 0.90: score += 1.0\n",
        "                elif sim >= 0.80: score += 0.5\n",
        "                else:             score -= 0.5\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    \"\"\"\n",
        "    Numeric scorer with robust parsing (commas, percents, simple fractions).\n",
        "    Uses relative error buckets.\n",
        "    \"\"\"\n",
        "    responses = [extract_response_text(c) for c in completions]\n",
        "    extracted = [\n",
        "        (m.group(1) if (m := match_numbers.search(r)) else None)\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        g = _to_float(guess)\n",
        "        t = _to_float(true_answer if isinstance(true_answer, str) else str(true_answer))\n",
        "        if g is None or t is None:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        if g == t:\n",
        "            scores.append(1.5)\n",
        "            continue\n",
        "\n",
        "        denom = max(abs(t), 1e-12)\n",
        "        rel_err = abs(g - t) / denom\n",
        "\n",
        "        if   rel_err <= 0.01: scores.append(1.0)   # within 1%\n",
        "        elif rel_err <= 0.05: scores.append(0.75)  # within 5%\n",
        "        elif rel_err <= 0.10: scores.append(0.5)   # within 10%\n",
        "        else:                  scores.append(0.0)\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "tK_Y_dnkwtgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, difflib, json\n",
        "\n",
        "# ------- helpers -------\n",
        "ALLOWED = {\n",
        "    \"cnn\", \"random_forest\", \"isolation_forest\", \"naivebayes\",\n",
        "    \"bert\", \"autoencoder\", \"dqn\", \"other\"\n",
        "}\n",
        "\n",
        "def _extract_response_text(completion):\n",
        "    \"\"\"Return assistant text from string / list-of-messages / dict shapes.\"\"\"\n",
        "    if isinstance(completion, str):\n",
        "        return completion\n",
        "    if isinstance(completion, list) and completion:\n",
        "        for msg in reversed(completion):\n",
        "            if isinstance(msg, dict) and msg.get(\"role\") in (\"assistant\", \"assistant_final\"):\n",
        "                c = msg.get(\"content\")\n",
        "                if isinstance(c, str):\n",
        "                    return c\n",
        "        last = completion[-1]\n",
        "        if isinstance(last, dict) and isinstance(last.get(\"content\"), str):\n",
        "            return last[\"content\"]\n",
        "    if isinstance(completion, dict):\n",
        "        for k in (\"content\", \"generated_text\", \"text\", \"response\"):\n",
        "            v = completion.get(k)\n",
        "            if isinstance(v, str):\n",
        "                return v\n",
        "    return str(completion)\n",
        "\n",
        "def _canon_model(s: str) -> str:\n",
        "    t = s.strip().lower()\n",
        "    # normalize common spellings\n",
        "    t = t.replace(\"-\", \" \").replace(\"_\", \" \")\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "\n",
        "    # map synonyms → canonical labels\n",
        "    if \"random\" in t and \"forest\" in t:   return \"random_forest\"\n",
        "    if \"isolation\" in t and \"forest\" in t:return \"isolation_forest\"\n",
        "    if \"naive\" in t and \"bayes\" in t:     return \"naivebayes\"\n",
        "    if t in (\"cnn\", \"convnet\", \"convolutional neural network\"): return \"cnn\"\n",
        "    if t.startswith(\"bert\"):              return \"bert\"\n",
        "    if \"autoencoder\" in t or \"auto encoder\" in t: return \"autoencoder\"\n",
        "    if t == \"dqn\" or \"deep q\" in t:       return \"dqn\"\n",
        "    if t == \"other\":                      return \"other\"\n",
        "\n",
        "    # plain canonicalization fallback\n",
        "    t = re.sub(r\"[^\\w]\", \"\", t)\n",
        "    if t == \"randomforest\":    return \"random_forest\"\n",
        "    if t == \"isolationforest\": return \"isolation_forest\"\n",
        "    if t == \"naivebayes\":      return \"naivebayes\"\n",
        "    if t in ALLOWED:           return t\n",
        "    return \"other\"\n",
        "\n",
        "_SPLIT = re.compile(r\"^\\s*(?P<model>[^-\\n]{1,80})\\s*-\\s*(?P<reason>.+?)\\s*$\", re.S)\n",
        "\n",
        "def _parse_pred(line: str):\n",
        "    \"\"\"Return (model_canon, reason_str, format_ok).\"\"\"\n",
        "    m = _SPLIT.match(line.strip())\n",
        "    if not m:\n",
        "        return (\"other\", line.strip(), False)\n",
        "    model_raw = m.group(\"model\")\n",
        "    reason    = m.group(\"reason\").strip()\n",
        "    return (_canon_model(model_raw), reason, True)\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def _sim(a: str, b: str) -> float:\n",
        "    return difflib.SequenceMatcher(None, _norm(a), _norm(b)).ratio()\n",
        "\n",
        "# ------- TEXT-ONLY REWARD -------\n",
        "def check_answer(prompts, completions, answer=None, label=None, rationale=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Scores purely text outputs of the form '<MODEL> - <REASON>'.\n",
        "    Prefers row keys:\n",
        "      - label: canonical gold model (e.g., 'random_forest')\n",
        "      - rationale: gold one-line reason\n",
        "      - answer: full string '<MODEL> - <REASON>' (fall back if label/rationale missing)\n",
        "    \"\"\"\n",
        "    # Build golds per-sample from whichever fields trainer passes\n",
        "    # TRL usually passes 'answer'; but your dataset also has 'label' + 'rationale'.\n",
        "    gold_answers   = answer\n",
        "    gold_labels    = label\n",
        "    gold_rationales = rationale\n",
        "\n",
        "    # If any are None, make them lists of None so zip works\n",
        "    if gold_answers is None:    gold_answers = [None] * len(completions)\n",
        "    if gold_labels is None:     gold_labels = [None] * len(completions)\n",
        "    if gold_rationales is None: gold_rationales = [None] * len(completions)\n",
        "\n",
        "    preds = [_extract_response_text(c) for c in completions]\n",
        "\n",
        "    scores = []\n",
        "    for pred, gold_ans, gold_lab, gold_rat in zip(preds, gold_answers, gold_labels, gold_rationales):\n",
        "        score = 0.0\n",
        "\n",
        "        # If we have the full gold 'answer' string: exact / normalized checks first\n",
        "        if isinstance(gold_ans, str) and gold_ans:\n",
        "            if pred == gold_ans:\n",
        "                scores.append(3.0)  # perfect\n",
        "                continue\n",
        "            if _norm(pred) == _norm(gold_ans):\n",
        "                score += 2.0\n",
        "\n",
        "        # Parse prediction into model + reason (format '<model> - <reason>')\n",
        "        pred_model, pred_reason, fmt_ok = _parse_pred(pred)\n",
        "        if fmt_ok:\n",
        "            score += 0.5   # bonus for correct format\n",
        "        else:\n",
        "            score -= 0.25  # small penalty for wrong format\n",
        "\n",
        "        # Decide gold model/rationale\n",
        "        gold_model = None\n",
        "        gold_reason = None\n",
        "\n",
        "        if isinstance(gold_lab, str) and gold_lab:\n",
        "            gold_model = gold_lab\n",
        "        elif isinstance(gold_ans, str) and gold_ans:\n",
        "            m = _SPLIT.match(gold_ans)\n",
        "            if m:\n",
        "                gold_model = _canon_model(m.group(\"model\"))\n",
        "                gold_reason = m.group(\"reason\").strip()\n",
        "\n",
        "        if isinstance(gold_rat, str) and gold_rat:\n",
        "            gold_reason = gold_rat\n",
        "\n",
        "        # Model correctness\n",
        "        if gold_model:\n",
        "            if pred_model == _canon_model(gold_model):\n",
        "                score += 1.5\n",
        "            else:\n",
        "                # partial credit if it's at least a valid label but wrong\n",
        "                score += 0.0  # (no extra credit)\n",
        "        else:\n",
        "            # if we don't know gold model, don't penalize\n",
        "            pass\n",
        "\n",
        "        # Reasoning similarity (if we have a target)\n",
        "        if gold_reason:\n",
        "            sim = _sim(pred_reason, gold_reason)\n",
        "            if   sim >= 0.92: score += 1.5\n",
        "            elif sim >= 0.85: score += 1.0\n",
        "            elif sim >= 0.70: score += 0.5\n",
        "            else:             score += 0.0\n",
        "        else:\n",
        "            # If we have no gold rationale, lightly reward non-empty reason\n",
        "            if len(pred_reason.split()) >= 4:\n",
        "                score += 0.25\n",
        "\n",
        "        # Guard against trivially short outputs\n",
        "        if len(pred.strip()) < 6:\n",
        "            score -= 0.25\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "niyyp3edylye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_completions = [\n",
        "    \"GPT-4 - Uses chain-of-thought to explain steps\",\n",
        "    \"BERT - Bidirectional masked LM pretraining\"\n",
        "]\n",
        "fake_answers = [\n",
        "    {\"model\": \"GPT-4\", \"reason\": \"uses chain-of-thought to explain steps\"},\n",
        "    {\"model\": \"BERT\", \"reason\": \"bidirectional masked LM pretraining\"}\n",
        "]\n",
        "print(check_answer(completions=fake_completions, answer=fake_answers))\n",
        "# Expect non-zero, varied numbers (not all 0)\n"
      ],
      "metadata": {
        "id": "eiFb1U-rtAcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "max_prompt_length = 256\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_torch_fused\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 4, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_seq_length - max_prompt_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 50,\n",
        "    save_steps = 50,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Rename your target to what TRL expects\n",
        "# if \"answer\" in dataset.column_names:\n",
        "#     dataset = dataset.rename_column(\"answer\", \"completion\")\n",
        "# if \"response\" in dataset.column_names and \"completion\" not in dataset.column_names:\n",
        "#     dataset = dataset.rename_column(\"response\", \"completion\")\n",
        "if \"label\" in dataset.column_names:\n",
        "    # either drop it OR keep it, but only if you ALSO have \"completion\"\n",
        "    dataset = dataset.remove_columns([\"label\"])\n",
        "\n",
        "# 2) Keep only the accepted keys for this mode: {\"prompt\",\"completion\"}\n",
        "# keep = {\"prompt\", \"completion\"}\n",
        "# dataset = dataset.remove_columns([c for c in dataset.column_names if c not in keep])\n",
        "\n",
        "# print(dataset.column_names)  # should be ['prompt', 'completion']\n"
      ],
      "metadata": {
        "id": "zcSqy775LAJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        check_answer,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/gemma-3-finetune/model.safetensors\")\n"
      ],
      "metadata": {
        "id": "xlYjoK9aHfqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"\"\"\n",
        "  \"event_id\": \"3b00489f-5c83-4b94-a0cc-54e19c790edc\",\n",
        "  \"timestamp\": \"2025-01-18T20:07:20\",\n",
        "  \"severity\": \"info\",\n",
        "  \"raw_log\": \"CEF:0|Carbon Black v7.8.0|SIEM|1.0|100|firewall|info| desc=Firewall drop SSH traffic from 100.11.91.183:51767 to 35.35.93.73:580 No additional info\",\n",
        "  \"user\": null,\n",
        "  \"action\": \"drop\",\n",
        "  \"description\": \"Firewall drop SSH traffic from 100.11.91.183:51767 to 35.35.93.73:580 No additional info\",\n",
        "  \"src_ip\": \"100.11.91.183\",\n",
        "  \"dst_ip\": \"35.35.93.73\",\n",
        "  \"category\": null\n",
        "\"\"\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"gemma-3\")  # Local saving\n",
        "tokenizer.save_pretrained(\"gemma-3\")\n",
        "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3-finetune`. Set `if False` to `if True` to let it run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "if True: # Change to True to save finetune!\n",
        "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRrqfyaRaXRL"
      },
      "source": [
        "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-B07m_HC5i7"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\n",
        "        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
        "        token = \"hf_...\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JMDDS0bC7jT"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX3xMj8hC6e0"
      },
      "outputs": [],
      "source": [
        "if True: # Change to True to save to GGUF\n",
        "    model.save_pretrained_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        tokenizer,\n",
        "        quantization_method = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CfqZW5rC9zv"
      },
      "source": [
        "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG6-dP0JC-G2"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload GGUF\n",
        "    model.push_to_hub_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        tokenizer,\n",
        "        quantization_method = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
        "        repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
        "        token = \"hf_...\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F2GVrPrDArH"
      },
      "source": [
        "Now, use the `gemma-3-finetune.gguf` file or `gemma-3-finetune-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "unsloth_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}